"""
ACE React Agent è®­ç»ƒç¨‹åº

ä½¿ç”¨ train_questions.json è®­ç»ƒ ACE React Agent çš„ Playbookï¼Œ
è‡ªåŠ¨å­¦ä¹ ç­–ç•¥å¹¶ä¿å­˜åˆ° ace_react_playbook.jsonã€‚
"""

import json
import sys
from pathlib import Path
from typing import List
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from ace_langgraph.ace_react_agent import ACEReActWorkflow, get_default_tools
from ace_langgraph.types import ReactQuestion


def load_train_questions(filepath: str) -> List[ReactQuestion]:
    """
    ä» JSON æ–‡ä»¶åŠ è½½è®­ç»ƒé—®é¢˜ã€‚
    
    å‚æ•°ï¼š
        filepath: JSON æ–‡ä»¶è·¯å¾„
        
    è¿”å›ï¼š
        ReactQuestion å¯¹è±¡åˆ—è¡¨
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    questions = []
    for item in data:
        question = ReactQuestion(
            question=item['question'],
            ground_truth=item['ground_truth'],
            context=item.get('context', '')
        )
        questions.append(question)
    
    return questions


def train_agent(
    training_file: str = "train_questions.json",
    playbook_path: str = "ace_react_playbook.json",
    model_name: str = "gpt-4o-mini",
    max_iterations: int = 15,
    verbose: bool = False,
    show_progress: bool = True
):
    """
    è®­ç»ƒ ACE React Agentã€‚
    
    å‚æ•°ï¼š
        training_file: è®­ç»ƒé—®é¢˜æ–‡ä»¶è·¯å¾„
        playbook_path: Playbook ä¿å­˜è·¯å¾„
        model_name: LLM æ¨¡å‹åç§°
        max_iterations: Agent æœ€å¤§è¿­ä»£æ¬¡æ•°
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹
        show_progress: æ˜¯å¦æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
    """
    import os
    
    # æ£€æŸ¥ API key
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    print("\n" + "="*70)
    print("ACE React Agent è®­ç»ƒç¨‹åº")
    print("="*70)
    print(f"\nè®­ç»ƒæ–‡ä»¶ï¼š{training_file}")
    print(f"Playbook è·¯å¾„ï¼š{playbook_path}")
    print(f"æ¨¡å‹ï¼š{model_name}")
    print(f"æœ€å¤§è¿­ä»£ï¼š{max_iterations}")
    print(f"è¯¦ç»†è¾“å‡ºï¼š{'æ˜¯' if verbose else 'å¦'}")
    print("="*70 + "\n")
    
    # åŠ è½½è®­ç»ƒé—®é¢˜
    try:
        train_questions = load_train_questions(training_file)
        print(f"âœ“ æˆåŠŸåŠ è½½ {len(train_questions)} ä¸ªè®­ç»ƒé—®é¢˜\n")
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°è®­ç»ƒæ–‡ä»¶ï¼š{training_file}")
        return
    except Exception as e:
        print(f"âŒ åŠ è½½è®­ç»ƒæ–‡ä»¶å¤±è´¥ï¼š{e}")
        return
    
    # åˆ›å»º ACE React Workflowï¼ˆä¼šè‡ªåŠ¨åŠ è½½å·²æœ‰çš„ Playbook æˆ–åˆ›å»ºæ–°çš„ï¼‰
    print("åˆå§‹åŒ–å·¥ä½œæµ...\n")
    workflow = ACEReActWorkflow(
        tools=get_default_tools(),
        model_name=model_name,
        max_iterations=max_iterations,
        playbook_path=playbook_path,  # æŒ‡å®š Playbook è·¯å¾„
        use_vector_retrieval=False,
        auto_save=True  # æ¯æ¬¡è¿è¡Œåè‡ªåŠ¨ä¿å­˜
    )
    
    # æ˜¾ç¤ºåˆå§‹çŠ¶æ€
    initial_stats = workflow.playbook.stats()
    print(f"åˆå§‹ Playbook çŠ¶æ€ï¼š")
    print(f"  - ç­–ç•¥æ•°ï¼š{initial_stats['total_strategies']}")
    print(f"  - åˆ†ç±»æ•°ï¼š{initial_stats['categories']}")
    print()
    
    # è®­ç»ƒç»Ÿè®¡
    training_stats = {
        'total_questions': len(train_questions),
        'correct': 0,
        'incorrect': 0,
        'errors': 0,
        'start_time': datetime.now()
    }
    
    # å¼€å§‹è®­ç»ƒ
    print("="*70)
    print("å¼€å§‹è®­ç»ƒ...")
    print("="*70 + "\n")
    
    for i, question in enumerate(train_questions, 1):
        if show_progress and not verbose:
            print(f"\n{'='*70}")
            print(f"è®­ç»ƒè¿›åº¦ï¼š{i}/{len(train_questions)}")
            print(f"{'='*70}")
            print(f"é—®é¢˜ï¼š{question.question}")
            print(f"æ­£ç¡®ç­”æ¡ˆï¼š{question.ground_truth}")
            print("-" * 70)
        
        try:
            # è¿è¡Œè®­ç»ƒï¼ˆä¼šè‡ªåŠ¨æ‰§è¡Œï¼šGenerator â†’ Evaluator â†’ Reflector â†’ Curatorï¼‰
            result = workflow.run(question, verbose=verbose)
            
            # ç»Ÿè®¡ç»“æœ
            evaluation = result.get("evaluation")
            if evaluation:
                if evaluation.is_correct:
                    training_stats['correct'] += 1
                    status = "âœ“ æ­£ç¡®"
                else:
                    training_stats['incorrect'] += 1
                    status = "âœ— é”™è¯¯"
                
                if show_progress and not verbose:
                    react_result = result.get("react_result")
                    agent_answer = react_result.answer if react_result else "æœªç”Ÿæˆç­”æ¡ˆ"
                    print(f"Agentç­”æ¡ˆï¼š{agent_answer[:80]}...")
                    print(f"è¯„ä¼°ï¼š{status}")
                    print(f"åé¦ˆï¼š{evaluation.feedback[:80]}...")
            
            # æ˜¾ç¤ºå­¦ä¹ åˆ°çš„æ–°ç­–ç•¥
            if show_progress and not verbose:
                curator_result = result.get('curator_result')
                if curator_result:
                    added = curator_result.added_count
                    updated = curator_result.updated_count
                    removed = curator_result.removed_count
                    marked = curator_result.marked_count
                    
                    changes = []
                    if added > 0:
                        changes.append(f"æ–°å¢ {added} ä¸ª")
                    if updated > 0:
                        changes.append(f"æ›´æ–° {updated} ä¸ª")
                    if removed > 0:
                        changes.append(f"ç§»é™¤ {removed} ä¸ª")
                    if marked > 0:
                        changes.append(f"æ ‡è®° {marked} ä¸ª")
                    
                    if changes:
                        print(f"ç­–ç•¥å˜åŒ–ï¼š{', '.join(changes)}")
                    else:
                        print(f"ç­–ç•¥å˜åŒ–ï¼šæ— å˜åŒ–")
            
            # æ˜¾ç¤ºå½“å‰è¿›åº¦
            if show_progress and not verbose:
                current_accuracy = (training_stats['correct'] / i * 100) if i > 0 else 0
                current_strategies = len(workflow.playbook)
                print(f"\nå½“å‰ç»Ÿè®¡ï¼š")
                print(f"  - æ­£ç¡®ç‡ï¼š{current_accuracy:.1f}% ({training_stats['correct']}/{i})")
                print(f"  - ç­–ç•¥æ•°ï¼š{current_strategies}")
                
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥ï¼š{e}")
            training_stats['errors'] += 1
            if verbose:
                import traceback
                traceback.print_exc()
    
    # è®­ç»ƒç»“æŸï¼Œæ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    training_stats['end_time'] = datetime.now()
    training_stats['duration'] = (training_stats['end_time'] - training_stats['start_time']).total_seconds()
    
    print("\n" + "="*70)
    print("è®­ç»ƒå®Œæˆï¼")
    print("="*70)
    
    # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡
    print(f"\nè®­ç»ƒç»Ÿè®¡ï¼š")
    print(f"  æ€»é—®é¢˜æ•°ï¼š{training_stats['total_questions']}")
    print(f"  æ­£ç¡®æ•°é‡ï¼š{training_stats['correct']} âœ“")
    print(f"  é”™è¯¯æ•°é‡ï¼š{training_stats['incorrect']} âœ—")
    print(f"  å¼‚å¸¸æ•°é‡ï¼š{training_stats['errors']} âš ï¸")
    print(f"  æ­£ç¡®ç‡ï¼š{training_stats['correct']/training_stats['total_questions']*100:.1f}%")
    print(f"  è®­ç»ƒæ—¶é•¿ï¼š{training_stats['duration']:.1f} ç§’")
    
    # æ˜¾ç¤º Playbook ç»Ÿè®¡
    final_stats = workflow.playbook.stats()
    print(f"\næœ€ç»ˆ Playbook çŠ¶æ€ï¼š")
    print(f"  ç­–ç•¥æ€»æ•°ï¼š{final_stats['total_strategies']}")
    print(f"  åˆ†ç±»æ•°é‡ï¼š{final_stats['categories']}")
    print(f"  æ ‡è®°ç»Ÿè®¡ï¼šâœ“{final_stats['tags']['helpful']} / âœ—{final_stats['tags']['harmful']} / ~{final_stats['tags']['neutral']}")
    print(f"  å¹³å‡å¾—åˆ†ï¼š{final_stats['avg_score']:.2f}")
    
    strategies_gained = final_stats['total_strategies'] - initial_stats['total_strategies']
    print(f"\næœ¬æ¬¡è®­ç»ƒï¼š")
    print(f"  æ–°å¢ç­–ç•¥ï¼š{strategies_gained} ä¸ª")
    
    # æ˜¾ç¤ºå‰ 10 ä¸ªé«˜åˆ†ç­–ç•¥
    if len(workflow.playbook) > 0:
        print(f"\nå‰ 10 ä¸ªé«˜åˆ†ç­–ç•¥ï¼š")
        print("-" * 70)
        top_strategies = workflow.playbook.get_top_strategies(n=10)
        
        for i, strategy in enumerate(top_strategies, 1):
            print(f"\n{i}. [{strategy.id}] åˆ†æ•°: {strategy.score}")
            print(f"   åˆ†ç±»: {strategy.category}")
            print(f"   å†…å®¹: {strategy.content[:100]}{'...' if len(strategy.content) > 100 else ''}")
            print(f"   æ ‡è®°: âœ“{strategy.helpful_count} / âœ—{strategy.harmful_count} / ~{strategy.neutral_count}")
    
    print(f"\nğŸ’¾ Playbook å·²ä¿å­˜åˆ°ï¼š{playbook_path}")
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼å¯ä»¥ä½¿ç”¨ test_ace_react.py æµ‹è¯•æ€§èƒ½")
    print()
    
    return workflow, training_stats


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ACE React Agent è®­ç»ƒç¨‹åº")
    parser.add_argument(
        "--training-file",
        default="train_questions.json",
        help="è®­ç»ƒé—®é¢˜æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼štrain_questions.jsonï¼‰"
    )
    parser.add_argument(
        "--playbook",
        default="ace_react_playbook.json",
        help="Playbook ä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤ï¼šace_react_playbook.jsonï¼‰"
    )
    parser.add_argument(
        "--model",
        default="gpt-4o-mini",
        help="LLM æ¨¡å‹åç§°ï¼ˆé»˜è®¤ï¼šgpt-4o-miniï¼‰"
    )
    parser.add_argument(
        "--max-iterations",
        type=int,
        default=15,
        help="Agent æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤ï¼š15ï¼‰"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†è¾“å‡º"
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="é™é»˜æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºè¿›åº¦"
    )
    
    args = parser.parse_args()
    
    train_agent(
        training_file=args.training_file,
        playbook_path=args.playbook,
        model_name=args.model,
        max_iterations=args.max_iterations,
        verbose=args.verbose,
        show_progress=not args.quiet
    )


if __name__ == "__main__":
    main()
